# <center>anafine 아나파잉</center>

#### <center>나현웅 김민우 박동휘 박종민 이원희 정재현</center>

# 목차
1. 프로젝트 사용 도구
2. 개발환경
3. 외부 서비스
4. 환경변수
5. 실행 및 배포
   - openresty
   - jenkins
        - backend
        - frontend
        - chatbot
   - data crawl
   - elasticsearch


## 1. 프로젝트 사용 도구
- 이슈 관리 : JIRA
- 형상 관리 : Gitlab
- 커뮤니케이션 : Notion, MatterMost
- 디자인 : Figma
- CI/CD : Gitlab-CI, Jenkins

## 2. 개발 환경
- VS Code : latest
- IntelliJ : latest
- NodeJS : 18.16.1
- SERVER AWS EC2 Ubuntu(20.04 LTS) T2 xlarge0
- MySQL : 8.0
- Spring Boot : 2.7.17
- jdk 11
- Next.js : 12.3.4
- React.js : 18.2.0

## 3. 외부 서비스
- Kakao API
- ChatGPT API
- serpAPI
- papagoAPI

## 4. 환경변수 (.env)
### 벡엔드
<pre><code>#backend
ELASTICSEARCH_HOST=k9a403a.p.ssafy.io
ELASTICSEARCH_PORT=9200
ELASTICSEARCH_USERNAME=elastic
ELASTICSEARCH_PASSWORD=wonhee
SPRING_DATASOURCE_URL=jdbc:mysql://k9a403.p.ssa0fy.io:3306/anafine
SPRING_DATASOURCE_USERNAME=A403
SPRING_DATASOURCE_PASSWORD=ssafy!A403
KAKAO_REST_API_KEY=0d0318f028971bea3git743a6a5c9bfc471
</code></pre>


### 프론트엔드
<pre><code>#frontend
NEXT_PUBLIC_KAKAOMAP_APPKEY=""
</code></pre>


### 데이터
<pre><code>#data
#공공데이터
PUBLIC_DATA_API_KEY = 

#MYSQL
DB_SERVER_URL = localhost
DB_PORT = 3306
DB_USERNAME = A403
DB_PASSWORD = ssafy!A403
DB_DATABASE_NAME = anafine

#구글드라이브 DB 베이스의 파일 ID
BASE_INFOS_FILE_DRIVE_ID = 1V8g9Xc_JqLFWUnVxRTVYAvQXU0p_uc9B
HOSPITAL_FILE_DRIVE_ID = 1jEuQiXHkH8FIvIyrXdUFkON9KjUpOPDB
CATEGORY_FILE_DRIVE_ID = 13dd2B9h5-nHxRntTR35kspmo076ih79q
TREATMENT_FILE_DRIVE_ID = 1hEFgcSvg346b8dRA1YnE-9ofE1Ur2x6F
HOSPITAL_TYPE_FILE_DRIVE_ID = 18Ni4HmjtbQNPt0a6idRc8-vd0p8gDnLj

#분리가 필요한 속성
#create, none = validate, update
DB_DDL_AUTO = create 
#True, False
FILE_OVERWRITE = False
</code></pre>

### 챗봇
<pre><code>#chatbot
OPENAI_API_KEY=
SERPAPI_API_KEY=
PAPAGO_CLIENT_ID=
PAPAGO_CLIENT_SECRET=
</code></pre>


## 5. 실행 및 배포

서버1 : openresty / jenkins / 백엔드 / 프론트엔드 / mysql(slave) / chatbot / filebeats <br/>
서버2 : elasticsearch / kibana / data crwaling server

### 5.1 openresty
<details>
   <summary>자세히</summary>
   
   #### 설치
<pre><code>
#기존 nginx stop
sudo systemctl disable nginx
sudo systemctl stop nginx

#ubuntu 20.04 기준
sudo apt-get -y install --no-install-recommends wget gnupg ca-certificates
wget -O - https://openresty.org/package/pubkey.gpg | sudo apt-key add -

echo "deb http://openresty.org/package/ubuntu $(lsb_release -sc) main" \
 | sudo tee /etc/apt/sources.list.d/openresty.list

sudo apt-get update

sudo apt-get -y install openresty

export PATH=/usr/local/openresty/bin:$PATH >> ~/.bashrc
source ~/bashrc

#test
resty -e 'print("hello, world")'
-> hello, world

mkdir ~/work
cd ~/work
mkdir logs/ conf/

$conf/nginx.conf

#nginx.conf
worker_processes  1;
error_log logs/error.log;
events {
    worker_connections 1024;
}
http {
    server {
        listen 8080;
        location / {
            default_type text/html;
            content_by_lua_block {
                ngx.say("<p>hello, world</p>")
            }
        }
    }
}
   
sudo openresty -p `pwd`/ -c work/conf/nginx.conf

curl http://localhost:8080/
<p>hello, world</p>
</code></pre>

#### 설정파일
##### nginx.conf
<pre><code>
worker_processes  1;
error_log /home/ubuntu/work/logs/error.log;
pid /home/ubuntu/work/logs/nginx.pid;
events {
    worker_connections 1024;
}
http {
    lua_shared_dict limit_traffic 10m;
    include /home/ubuntu/work/conf/conf.d/*.conf;
}
</code></pre>

##### default.conf
<pre><code>
lua_package_path "/home/ubuntu/work/conf/lua/?.lua;;";

init_worker_by_lua_block {
    require("limit_traffic").init()
}

limit_req_zone $binary_remote_addr zone=chatbot_limit:10m rate=1r/s;

server {
    listen 80;
    server_name k9a403.p.ssafy.io anafine.com www.anafine.com;
    return 301 https://www.anafine.com$request_uri;
}

server {
    listen 443 ssl;
    server_name www.anafine.com;
    ssl_certificate /etc/letsencrypt/live/www.anafine.com/fullchain.pem; # managed by Certbot
    ssl_certificate_key /etc/letsencrypt/live/www.anafine.com/privkey.pem; # managed by Certbot
    include /etc/letsencrypt/options-ssl-nginx.conf; # managed by Certbot
    ssl_dhparam /etc/letsencrypt/ssl-dhparams.pem; # managed by Certbot

    include /etc/nginx/conf.d/service-url.inc;

    location /api {
        rewrite ^/api(/.*)$ $1 break;
        proxy_pass $service_url;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_redirect off;

        proxy_buffer_size          128k;
        proxy_buffers              4 256k;
        proxy_busy_buffers_size    256k;

    }

    include /etc/nginx/conf.d/front-service-url.inc;
    location / {
        proxy_pass $front_service_url;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_redirect off;
    }


        location /chat {
            content_by_lua_block {
                local requested_uri = ngx.var.uri
                ngx.log(ngx.ERR, "Requested URI: ", requested_uri)

                local modified_uri = "/to" .. requested_uri

                ngx.log(ngx.ERR, "Modified URI: ", modified_uri)

                require("limit_traffic").limit()
                ngx.exec(modified_uri)
            }
        }

    include /etc/nginx/conf.d/chatbot-service-url.inc;
    location /to {
        internal;
        rewrite ^/to(/.*)$ $1 break;


        log_by_lua_block {
           ngx.log(ngx.ERR, "Accessing /to location");
        }
        proxy_pass $chatbot_service_url;
        proxy_http_version 1.1;
        proxy_set_header Upgrade $http_upgrade;
        proxy_set_header Connection "upgrade";
        proxy_set_header Host $host;
        proxy_set_header X-Forwarded-Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_redirect off;
    }
}   
</code></pre>

***include files*** </br>
service-url.inc/front-service-url.inc/chatbot-service-url.inc </br>
inc파일은 무중단 배포를 위한 파일로 jenkins pipeline에서 포트 확인을 통해 수정됨 </br>
ex) set $service_url http://localhost:8080 -> set $service_url http://localhost:8081 
 
##### jenkins.conf
<pre><code>
upstream jenkins {
        keepalive 32; # keepalive connections
        server localhost:9090;
}

# Required for Jenkins websocket agents
map $http_upgrade $connection_upgrade {
        default upgrade;
        '' close;
}

server {
        listen 80; # Listen on port 80 for IPv4 requests
        server_name jenkins.anafine.com;

        root            /var/run/jenkins/war/;
        access_log      /var/log/nginx/jenkins.access.log;
        error_log       /var/log/nginx/jenkins.error.log;

        ignore_invalid_headers off;

        location ~ "^/static/[0-9a-fA-F]{8}\/(.*)$" {
                rewrite "^/static/[0-9a-fA-F]{8}\/(.*)" /$1 last;
        }

        location /userContent {
                root /var/lib/jenkins/;
                if (!-f $request_filename){
                        rewrite (.*) /$1 last;
                        break;
                }

                sendfile on;
        }

        location / {
                sendfile off;
                proxy_pass              http://jenkins;
                proxy_redirect          default;
                proxy_http_version      1.1;

                proxy_set_header Connection             $connection_upgrade;
                proxy_set_header Upgrade                $http_upgrade;
                proxy_set_header Host                   $http_host;
                proxy_set_header X-Real-IP              $remote_addr;
                proxy_set_header X-Forwarded-For        $proxy_add_x_forwarded_for;
                proxy_set_header X-Forwarded-Proto      $scheme;
                proxy_max_temp_file_size 0;

                client_max_body_size            10m;
                client_body_buffer_size         128k;
                proxy_connect_timeout           90;
                proxy_send_timeout              90;
                proxy_read_timeout              90;
                proxy_buffering                 off;
                proxy_request_buffering         off;
                proxy_set_header Connection     "";
        }
}
</code></pre>

##### limit_traffic.lua
<pre><code>
local ngx = ngx

local M = {}

-- Ensure that ngx.shared.limit_traffic is defined
if not ngx.shared.limit_traffic then
    ngx.shared.limit_traffic = ngx.shared("limit_traffic")
end

local limit_traffic = ngx.shared.limit_traffic

function M.init()
    -- Check if the key 'daily_limit' already exists
    local value, err = limit_traffic:get("daily_limit")
    if value then
        return
    end

    -- Attempt to add the shared memory zone
    local ok, err = limit_traffic:add("daily_limit", 10)
    if not ok then
        ngx.log(ngx.ERR, "Failed to initialize shared memory: ", err)
    end

    M.reset()
    -- Calculate the time until the next reset (5 minutes)
--    local now = ngx.now()
--    local reset_time = math.floor(now) + 60 - now % 60
--    ngx.log(ngx.ERR, "[timer] now : ", now)
    -- Set a timer to reset the daily limit every 5 minutes
--    ngx.timer.at(reset_time, M.reset)
end


function M.limit()
    local key = ngx.var.binary_remote_addr
    local limit = 10

    local remaining, err = limit_traffic:incr(key, 1, 0)
    if not remaining then
        ngx.log(ngx.ERR, "Failed to increment counter for key: ", key, ", error: ", err)
        ngx.exit(500)
    elseif remaining > limit then
        ngx.status = 429
        ngx.say("Too Many Requests")
        ngx.exit(ngx.HTTP_TOO_MANY_REQUESTS)
        ngx.log(ngx.ERR, "Limit exceeded for key: ", key)
        --ngx.exit(429)
    end
end

function M.reset(premature)
    ngx.log(ngx.ERR, "reset activated")
    if not premature then
        limit_traffic:flush_all()
        flushed = limit_traffic:flush_expired()
        ngx.log(ngx.ERR, "flushed : ", flushed)
        if not flushed then
            ngx.log(ngx.ERR, "Failed to reset daily limit: ", err)
        end
    end
    ngx.timer.at(3600, M.reset)
end

return M
</code></pre>

</details>

### 5.2 jenkins
backend/ frontend/ chatbot 3개의 파이프라인 </br>
jenkins/workspace 하위에 config dir 생성 후 도커컴포즈, .env 등 배포에 필요한 파일 보관

<details>
   <summary>자세히</summary>

   #### backend pipeline
<pre><code>
pipeline {
    agent any
    
    environment {
        running_port = null
        available_port = null
        res = null
    }

    stages {
        
        stage('Clone Repository') {
            steps {
                echo "Branch : develop"
                echo "Clone repository"
                git url: "https://lab.ssafy.com/s09-final/S09P31A403.git", branch: "develop", credentialsId: "anafine"
            }
        }
        
        stage('check if BE') {
            steps {
                script {
                    res = sh(script : 'git whatchanged -1 | grep backend', returnStatus: true,returnStdout: true)
                }
            }
        }
        
        stage('port check') {
            steps {
                script {
                    def result = sh(script: 'ssh -i ../config/backend/K9A403T.pem ubuntu@k9a403.p.ssafy.io curl -w "%{http_code}" -s -o /dev/null localhost:8080/actuator/health', returnStatus: true, returnStdout: true)
                    echo "$result"
                    if (result == 0) {
                        echo "running port : 8080"
                        running_port = 8080
                        available_port = 8081
                    } else {
                        echo "running port : 8081"
                        running_port = 8081
                        available_port = 8080
                    }
                }
            }
        }
        
        stage("Set environment / gradle build") {
            when { expression { res == 0 }}
            steps {
                echo "Copy require file to pipeline folder"
                sh "cp ../config/backend/docker-compose-${running_port}.yml ./backend"
                sh "cp ../config/backend/docker-compose-${available_port}.yml ./backend"
                sh 'cp ../config/backend/Dockerfile ./backend'
                sh 'cp ../config/backend/.env ./backend'
                
                dir('./backend') {
                    sh "chmod +x ./gradlew"
                    sh "./gradlew clean"
                    sh "./gradlew build -x test"
                }
            }
        }
        
        stage('Docker down') {
            when { expression { res == 0 }}
            steps {
                dir('./backend') {
                    echo "Docker compose down"
                    sh "docker-compose -f docker-compose-${available_port}.yml down --rmi all"
                }
                
            }
        }

        stage('Docker build') {
            when { expression { res == 0 }}
             steps {
                    echo "docker compose build"
                    sh "docker-compose -f ./backend/docker-compose-${available_port}.yml build --no-cache"
                }
                post {
                    success {
                        echo "Success to build"
                    }
                    failure {
                        echo "Docker build failed. clear unused file"
                        sh "docker system prune -f"
                        error 'pipeline aborted'
                    }
              }
        }
        
        stage('Docker up') {
            when { expression { res == 0 }}
            steps {
                dir('./backend') {
                    echo "docker compose up"
                    sh "docker-compose -f docker-compose-${available_port}.yml up -d"
                }
            }
        }
        
        stage('health check') {
            when { expression { res == 0 }}
            steps {
                script {
                    for (int i = 0; i < 20; i++) {
                        def result = sh(script: "ssh -i ../config/backend/K9A403T.pem ubuntu@k9a403.p.ssafy.io curl -w %{http_code} -s -o /dev/null localhost:${available_port}/actuator/health", returnStatus: true, returnStdout: true)
                        if(result == 0) {
                            echo 'health check success!'
                            break
                        }
                        echo "The server is not alive yet. Retry health check in 5 seconds..."
                        sleep(5)
                        if(i == 19) sh 'exit 1'
                    }
                }
            }
        }
        
        stage('nginx port setting') {
            when { expression { res == 0 }}
            steps {
                echo 'service_url 수정'
                sh "ssh -i ../config/backend/K9A403T.pem ubuntu@K9a403.p.ssafy.io 'echo \"set \\\$service_url http://127.0.0.1:${available_port};\" | sudo tee /etc/nginx/conf.d/service-url.inc'"

                echo 'nginx reload'
                sh "ssh -i ../config/backend/K9A403T.pem ubuntu@K9a403.p.ssafy.io 'sudo openresty -p `pwd` -s reload -c work/conf/nginx.conf'"
                
              
            }
        }
        
        
        stage('Docker down older server') {
            when { expression { res == 0 }}
            steps {
                dir('./backend') {
                    echo "Docker compose down"
                    sh "docker-compose -f docker-compose-${running_port}.yml down --rmi all"
                }
            }
            post {
                success {
                	script {
                        def Author_ID = sh(script: "git show -s --pretty=%an", returnStdout: true).trim()
                        def Author_Name = sh(script: "git show -s --pretty=%ae", returnStdout: true).trim()
                        mattermostSend (color: 'good', 
                        message: "빌드 성공(now running on port ${available_port}) : ${env.JOB_NAME} #${env.BUILD_NUMBER} by ${Author_ID}(${Author_Name})\n(<${env.BUILD_URL}|Details>)", 
                        endpoint: 'https://meeting.ssafy.com/hooks/a57sm7gnpjy1mxmwpi4qmou37w', 
                        channel: 'A403-private'
                        )
                    }
                }
            }
        }
    }

    post {
        failure {
        	script {
                def Author_ID = sh(script: "git show -s --pretty=%an", returnStdout: true).trim()
                def Author_Name = sh(script: "git show -s --pretty=%ae", returnStdout: true).trim()
                mattermostSend (color: 'danger', 
                message: "[BACKEND] 빌드 실패(still running on port ${running_port}): ${env.JOB_NAME} #${env.BUILD_NUMBER} by ${Author_ID}(${Author_Name})\n(<${env.BUILD_URL}|Details>)", 
                endpoint: 'https://meeting.ssafy.com/hooks/a57sm7gnpjy1mxmwpi4qmou37w', 
                channel: 'A403-private'
                )
            }
        }
    }
}

</code></pre>
#### frontend pipeline
<pre><code>
pipeline {
    agent any
    
    environment {
        running_port = null
        available_port = null
        res = null
    }

    stages {
        
        stage("Clone Repository") {
            steps {
                echo "Branch : develop"
                echo "Clone repository"
                git url: "https://lab.ssafy.com/s09-final/S09P31A403.git", branch: "develop", credentialsId: "anafine"
            }
        }
        
        stage('check if FE commit') {
            steps {
                script {
                    res = sh(script : 'git whatchanged -1 | grep frontend',returnStatus: true, returnStdout: true)
                }
            }
        }
        
         stage('port check') {
            steps {
                script {
                    def result = sh(script: 'ssh -i ../config/backend/K9A403T.pem ubuntu@k9a403.p.ssafy.io curl -w "%{http_code}" -s -o /dev/null localhost:3000', returnStatus: true, returnStdout: true)
                    echo "$result"
                    if (result == 0) {
                        echo "running port : 3000"
                        running_port = 3000
                        available_port = 3001
                    } else {
                        echo "running port : 3001"
                        running_port = 3001
                        available_port = 3000
                    }
                }
            }
        }
        
        
        
        stage("Set environment") {
            when { expression { res == 0 }}
            steps {
                echo "Copy require file to pipeline folder"
                sh "cp ../config/frontend/docker-compose-3000.yml ./frontend/"
                sh "cp ../config/frontend/docker-compose-3001.yml ./frontend/"
                sh "cp ../config/frontend/.env ./frontend/"
                sh "cp ../config/frontend/Dockerfile ./frontend/"
            }
        }

        
        stage("Docker down") {
            when { expression { res == 0 }}
            steps {
                echo "Docker compose down"
                sh "docker-compose -f ./frontend/docker-compose-${available_port}.yml down --rmi all"
            }
        }
        
        stage('Docker build') {
            when { expression { res == 0 }}
             steps {
                    echo "docker compose build"
                    sh "docker-compose -f ./frontend/docker-compose-${available_port}.yml build --no-cache"
                }
                post {
                    success {
                        echo "Success to build"
                    }
                    failure {
                        echo "Docker build failed. clear unused file"
                        sh "docker system prune -f"
                        error 'pipeline aborted'
                    }
              }
        }
        
        stage('Docker up') {
            when { expression { res == 0 }}
            steps {
                dir('./frontend') {
                    echo "docker compose up"
                    sh "docker-compose -f docker-compose-${available_port}.yml up -d"
                }
            }
        }
        
        stage('nginx port setting') {
            when { expression { res == 0 }}
            steps {
                echo 'front service_url 수정'
                sh "ssh -i ../config/backend/K9A403T.pem ubuntu@K9a403.p.ssafy.io 'echo \"set \\\$front_service_url http://127.0.0.1:${available_port};\" | sudo tee /etc/nginx/conf.d/front-service-url.inc'"

                echo 'nginx reload'
                sh "ssh -i ../config/backend/K9A403T.pem ubuntu@K9a403.p.ssafy.io 'sudo openresty -p `pwd` -s reload -c work/conf/nginx.conf'"
                
              
            }
        }
        
        stage('Docker down older server') {
            when { expression { res == 0 }}
            steps {
                dir('./frontend') {
                    echo "Docker compose down"
                    sh "docker-compose -f docker-compose-${running_port}.yml down --rmi all"
                }
            }
            post {
                success {
                	script {
                        def Author_ID = sh(script: "git show -s --pretty=%an", returnStdout: true).trim()
                        def Author_Name = sh(script: "git show -s --pretty=%ae", returnStdout: true).trim()
                        mattermostSend (color: 'good', 
                        message: "빌드 성공(now running on port ${available_port}) : ${env.JOB_NAME} #${env.BUILD_NUMBER} by ${Author_ID}(${Author_Name})\n(<${env.BUILD_URL}|Details>)", 
                        endpoint: 'https://meeting.ssafy.com/hooks/a57sm7gnpjy1mxmwpi4qmou37w', 
                        channel: 'A403-private'
                        )
                    }
                }
            }
        }
    }
    
   post {
        failure {
        	script {
                def Author_ID = sh(script: "git show -s --pretty=%an", returnStdout: true).trim()
                def Author_Name = sh(script: "git show -s --pretty=%ae", returnStdout: true).trim()
                mattermostSend (color: 'danger', 
                message: "[FRONTEND] 빌드 실패(still running on port ${running port}): ${env.JOB_NAME} #${env.BUILD_NUMBER} by ${Author_ID}(${Author_Name})\n(<${env.BUILD_URL}|Details>)", 
                endpoint: 'https://meeting.ssafy.com/hooks/a57sm7gnpjy1mxmwpi4qmou37w', 
                channel: 'A403-private'
                )
            }
        }
    }
    
}
</code></pre>

#### chatbot pipeline
<pre><code>
pipeline {
    agent any
    
    environment {
        running_port = null
        available_port = null
        res = null
    }

    stages {
        
        stage('Clone Repository') {
            steps {
                echo "Branch : develop"
                echo "Clone repository"
                git url: "https://lab.ssafy.com/s09-final/S09P31A403.git", branch: "develop", credentialsId: "anafine"
            }
        }
        
        stage('check if chatbot') {
            steps {
                script {
                    res = sh(script : 'git whatchanged -1 | grep chatbot', returnStatus: true,returnStdout: true)
                }
            }
        }
        
        stage('port check') {
            steps {
                script {
                    def result = sh(script: 'ssh -i ../config/backend/K9A403T.pem ubuntu@k9a403.p.ssafy.io curl -w "%{http_code}" -s -o /dev/null localhost:5000/chat/chatbot/health', returnStatus: true, returnStdout: true)
                    echo "$result"
                    if (result == 0) {
                        echo "running port : 5000"
                        running_port = 5000
                        available_port = 5001
                    } else {
                        echo "running port : 5001"
                        running_port = 5001
                        available_port = 5000
                    }
                }
            }
        }
        
        stage("Set environment / gradle build") {
            when { expression { res == 0 }} 
            steps {
                echo "Copy require file to pipeline folder"
                sh "cp ../config/chatbot/docker-compose-${running_port}.yml ./chatbot"
                sh "cp ../config/chatbot/docker-compose-${available_port}.yml ./chatbot"
                sh 'cp ../config/chatbot/Dockerfile ./chatbot'
                sh 'cp ../config/chatbot/.env ./chatbot'
            }
        }
        
        stage('Docker down') {
            when { expression { res == 0 }}
            steps {
                dir('./chatbot') {
                    echo "Docker compose down"
                    sh "docker-compose -f docker-compose-${available_port}.yml down --rmi all"
                }
                
            }
        }

        stage('Docker build') {
            when { expression { res == 0 }}
             steps {
                    echo "docker compose build"
                    sh "docker-compose -f ./chatbot/docker-compose-${available_port}.yml build --no-cache"
                }
                post {
                    success {
                        echo "Success to build"
                    }
                    failure {
                        echo "Docker build failed. clear unused file"
                        sh "docker system prune -f"
                        error 'pipeline aborted'
                    }
              }
        }
        
        stage('Docker up') {
            when { expression { res == 0 }}
            steps {
                dir('./chatbot') {
                    echo "docker compose up"
                    sh "docker-compose -f docker-compose-${available_port}.yml up -d"
                }
            }
        }
        
        stage('health check') {
            when { expression { res == 0 }}
            steps {
                script {
                    for (int i = 0; i < 20; i++) {
                        def result = sh(script: "ssh -i ../config/backend/K9A403T.pem ubuntu@k9a403.p.ssafy.io curl -w %{http_code} -s -o /dev/null localhost:${available_port}/chat/chatbot/health", returnStatus: true, returnStdout: true)
                        if(result == 0) {
                            echo 'health check success!'
                            break
                        }
                        echo "The server is not alive yet. Retry health check in 5 seconds..."
                        sleep(5)
                        if(i == 19) sh 'exit 1'
                    }
                }
            }
        }
        
        stage('nginx port setting') {
            when { expression { res == 0 }}
            steps {
                echo 'service_url 수정'
                sh "ssh -i ../config/backend/K9A403T.pem ubuntu@K9a403.p.ssafy.io 'echo \"set \\\$chatbot_service_url http://127.0.0.1:${available_port};\" | sudo tee /etc/nginx/conf.d/chatbot-service-url.inc'"

                echo 'nginx reload'
                sh "ssh -i ../config/backend/K9A403T.pem ubuntu@K9a403.p.ssafy.io 'sudo openresty -p `pwd` -s reload -c work/conf/nginx.conf'"
                
              
            }
        }
        
        
        stage('Docker down older server') {
            when { expression { res == 0 }}
            steps {
                dir('./chatbot') {
                    echo "Docker compose down"
                    sh "docker-compose -f docker-compose-${running_port}.yml down --rmi all"
                }
            }
            post {
                success {
                	script {
                        def Author_ID = sh(script: "git show -s --pretty=%an", returnStdout: true).trim()
                        def Author_Name = sh(script: "git show -s --pretty=%ae", returnStdout: true).trim()
                        mattermostSend (color: 'good', 
                        message: "[CHATBOT] 빌드 성공(now running on port ${available_port}) : ${env.JOB_NAME} #${env.BUILD_NUMBER} by ${Author_ID}(${Author_Name})\n(<${env.BUILD_URL}|Details>)", 
                        endpoint: 'https://meeting.ssafy.com/hooks/a57sm7gnpjy1mxmwpi4qmou37w', 
                        channel: 'A403-private'
                        )
                    }
                }
            }
        }
    }

    post {
        failure {
        	script {
                def Author_ID = sh(script: "git show -s --pretty=%an", returnStdout: true).trim()
                def Author_Name = sh(script: "git show -s --pretty=%ae", returnStdout: true).trim()
                mattermostSend (color: 'danger', 
                message: "[CHATBOT] 빌드 실패(still running on port ${running port}): ${env.JOB_NAME} #${env.BUILD_NUMBER} by ${Author_ID}(${Author_Name})\n(<${env.BUILD_URL}|Details>)", 
                endpoint: 'https://meeting.ssafy.com/hooks/a57sm7gnpjy1mxmwpi4qmou37w', 
                channel: 'A403-private'
                )
            }
        }
    }
}
</code></pre>
</details>


### 5.3 data
<details>
   <summary>자세히</summary>

   <pre><code>
   ...

   #dump.sql
   $ docker exec anafine-master-db sh -c 'mysqldump --default-character-set=utf8mb4 -u root -p"{password}" anafine > /ananfine_db_dump_1.sql'
   $ docker cp anafine-master-db:/ananfine_db_dump_1.sql /home/ubuntu/mysql_data/
   $ docker exec anafine-master-db rm /ananfine_db_dump_1.sql
   </code></pre>
   
</details>

### 5.4 elasticsearch

<details>
   <summary>자세히</summary>

   #### 설치 및 환경설정
<pre><code>
$ useradd elastic -m -b /home/elastic -d /home/elastic -s /bin/bash
$ apt-get install sudo
$ visudo
$ elastic ALL=(ALL) NOPASSWD: ALL

## 자바 설치 및 환경변수 설정
$ sudo apt-get install openjdk-11-jdk
$ which java or whereis java
$ readlink -f {위 명령어에 나온 경로}
$ echo 'export JAVA_HOME="{위 명령어에 나온 jdk경로}"' >> ~/.bash_rc
$ echo 'export ES_JAVA_HOME="{위 명령어에 나온 jdk경로}"' >> ~/.bash_rc  #elasticsearch에서 요구하는 환경변수 
$ source ~/.bashrc
$ echo 'export PATH="${PATH}:${JAVA_HOME}/bin"' >> ~/.bash_rc
$ source ~/.bashrc
$ echo {위에서 등록한 변수명} #환경변수 등록이 제대로 되었는지 확인

#elasticsearch tarball
$ wget https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-7.15.0-linux-x86_64.tar.gz
$ tar -xvzf elasticsearch-7.15.0-linux-x86_64.tar.gz
$ cd elasticsearch-7.15.0
$ bin/elasticsearch -d -p PID
$ pkill -F PID


#elasticsearch docker-compose with kibana
version: '3.7'
services:
  es:
    image: docker.elastic.co/elasticsearch/elasticsearch:7.15.0
    container_name: es-single-node
    environment:
      - node.name=single
      - cluster.name=standalone
      - discovery.type=single-node
      - ELASTIC_PASSWORD=wonhee
      - xpack.security.enabled=true
    volumes:
            - data:/usr/share/elasticsearch/data:rw
            - log:/usr/share/elasticsearch/log:rw
    ports:
      - 9200:9200
    networks:
      - es-bridge
  kibana:
    image: docker.elastic.co/kibana/kibana:7.15.0
    container_name: kibana
    ports:
      - 5601:5601
    environment:
      - ELASTICSEARCH_HOSTS=["http://es:9200"]
      - ELASTICSEARCH_USERNAME=elastic
      - ELASTICSEARCH_PASSWORD=wonhee
    depends_on:
      - es
    networks:
      - es-bridge

volumes:
  data:
    driver: local
  log:
    drive: local
networks:
  es-bridge:
    driver: bridge

#plugin install(재시작 필요)
$ bin/elasticsearch-plugin install analysis-nori
$ bin/elasticsearch-plugin install https://github.com/HowookJeong/elasticsearch-analysis-arirang/releases/download/7.15.0/elasticsearch-analysis-arirang-7.15.0.zip

</code></pre>

#### setting/mapping/data insert
비급여항목에 대한 검색 자동완성을 위해 경로에 대해 초성, 자모, 한영변환

treatment.json : 비급여 항목 데이터 파싱
![image-1.png](./image.png)


<pre><code>


curl --location --request PUT 'http://localhost:9200/treatment' -u elastic:wonhee \
--header 'Content-Type: application/json' \
--data-raw '{
  "settings": {
    "index": {
      "number_of_shards": 1,
      "number_of_replicas": 0,
      "max_ngram_diff": 30
    },
    "analysis": {
      "analyzer": {
        "analyzer-treatment": {
          "type": "custom",
          "tokenizer": "arirang_tokenizer"
        },        
        "ngram-treatment": {
          "type": "custom",
          "tokenizer": "partial",
          "filter": [
            "lowercase"
          ]
        },
        "edge-front-treatment": {
          "type": "custom",
          "tokenizer": "edgefront",
          "filter": [
            "lowercase"
          ]
        },
        "edge-back-treatment": {
          "type": "custom",
          "tokenizer": "edgeback",
          "filter": [
            "lowercase"
          ]
        }
      },
      "tokenizer": {
        "partial": {
          "type": "ngram",
          "min_gram": 1,
          "max_gram": 30,
          "token_chars": [
            "letter",
            "digit"
          ]
        },
        "edgefront": {
          "type": "edge_ngram",
          "min_gram": 1,
          "max_gram": 30,
          "token_chars": [
            "letter",
            "digit"
          ]
        },
        "edgeback": {
          "type": "edge_ngram",
          "min_gram": 1,
          "max_gram": 30,
          "size": "back",
          "token_chars": [
            "letter",
            "digit"
          ]
        }
      },
      "normalizer": {
        "normalizer-treatment": {
          "type": "custom",
          "filter": [
            "lowercase"
          ]
        }
      }
    }
  },
    "mappings": {
    "_source": {
      "excludes": [
        "chosung",
        "jamo",
        "engtokor"
      ]
    },
    "properties": {
      "treatmentId": {
        "type": "keyword",
        "index": false
      },
      "path": {
        "type": "text",
        "analyzer": "analyzer-treatment",
        "fields" : {
          "exact": {
            "type": "keyword",
            "normalizer": "normalizer-treatment"
          },
          "front": {
            "type": "text",
            "analyzer": "edge-front-treatment"
          },
          "back": {
            "type": "text",
            "analyzer": "edge-back-treatment"
          },
          "partial": {
            "type": "text",
            "analyzer": "ngram-treatment"
          }
        }
      },
      "chosung": {
        "type": "text",
        "analyzer": "edge-front-treatment",
        "fields" : {
          "exact": {
            "type": "keyword",
            "normalizer": "normalizer-treatment"
          },          
          "back": {
            "type": "text",
            "analyzer": "edge-back-treatment"
          },
          "partial": {
            "type": "text",
            "analyzer": "ngram-treatment"
          }
        }
      },
      "jamo": {
        "type": "text",
        "analyzer": "edge-front-treatment",
        "fields" : {
          "exact": {
            "type": "keyword",
            "normalizer": "normalizer-treatment"
          },          
          "back": {
            "type": "text",
            "analyzer": "edge-back-treatment"
          },
          "partial": {
            "type": "text",
            "analyzer": "ngram-treatment"
          }
        }
      },
      "engtokor": {
        "type": "text",
        "analyzer": "edge-front-treatment",
        "fields" : {
          "exact": {
            "type": "keyword",
            "normalizer": "normalizer-treatment"
          },          
          "back": {
            "type": "text",
            "analyzer": "edge-back-treatment"
          },
          "partial": {
            "type": "text",
            "analyzer": "ngram-treatment"
          }
        }
      },
      "name": {
        "type": "keyword",
        "normalizer": "normalizer-treatment"
      },
      "info": {
        "type": "keyword",
        "index": false
      }
    }
  }
}'



$ curl -s -H "Content-Type: application/x-ndjson" -XPOST localhost:9200/_bulk -u elastic:wonhee --data-binary "@treatment.json"; echo
</code></pre>

   
</details>






